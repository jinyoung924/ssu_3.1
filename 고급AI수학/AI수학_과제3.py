# -*- coding: utf-8 -*-
"""과제3_20211111_홍길동.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yxD1JVX2Yyk4bs265tq2udkGT6BvlQ2-

# **2024 고급 AI 수학  - 과제 3** (2025년 5월 20일 생성)

## **과제 3 가이드 라인**
### 1. 코드 작성 시 Python Library는 'numpy'와 'matplotlob.pyplot'만 사용한다.
### 2. 문제1~문제4는 필수 제출, 문제5는 Optional
### 3. 평가는 10점 만점, 기본점수 2점 + 8점 (문제1~문제4 까지 문제당 2점 씩 가산)
### 4. 제출 기한은 2025년 5월 28일(수요일) 24시
### 5. 과제 파일명은 "과제3_학번_이름.ipynb"로 LMS 시스템에 업로드 한다.
"""



"""## **문제 1: 선형 회귀(Linear Regression)**

### 다음과 같이 직선 $𝑦=0.5𝑥+0.8$ 을 기준으로 표준편차가 0.25인 가우시안 노이즈를 갖는 데이터가 50개 주어졌다고 하자.
### 주어진 데이터에 대한 최소제곱법을 정의하고 경사하강법을 적용하여 추정 직선 추세선을 찾아라.
### 단, Step size (Learnin rate)를 0.25로 정의하고 최대 50회(max iteration = 50) 반복한 결과를 출력하라.

"""

# [1단계] 데이터 생성 + 전체 값 출력
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

x = np.random.rand(50)
true_y = 0.5 * x + 0.8
noise = np.random.normal(0, 0.25, size=x.shape)
y = true_y + noise

# 전체 50개 x, y 출력
print("📋 전체 x, y 데이터 출력:")
for i in range(50):
    print(f"{i+1:2d}: x = {x[i]:.4f}, y = {y[i]:.4f}")

# 산점도 시각화
plt.scatter(x, y, label='Noisy Data')
plt.plot(x, true_y, '--', label='True Line (y = 0.5x + 0.8)')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Generated Data')
plt.legend()
plt.grid(True)
plt.show()

"""#### [단계 1] Linear Regression 훈련 Data 생성 및 산점도 그리기"""

# [1단계] 데이터 생성 + 전체 값 출력
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

x = np.random.rand(50)
true_y = 0.5 * x + 0.8
noise = np.random.normal(0, 0.25, size=x.shape)
y = true_y + noise

# 전체 50개 x, y 출력
print("📋 전체 x, y 데이터 출력:")
for i in range(50):
    print(f"{i+1:2d}: x = {x[i]:.4f}, y = {y[i]:.4f}")

# 산점도 시각화
plt.scatter(x, y, label='Noisy Data')
plt.plot(x, true_y, '--', label='True Line (y = 0.5x + 0.8)')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Generated Data')
plt.legend()
plt.grid(True)
plt.show()

"""#### [단계 2]
### - Cost Function 정의
### - 경사하강법 초기화
### - 반복 루프를 이용한 경사하강법 업데이트 및 Cost Function (MSE) 평가

"""

# 초기값
w = 0.0
b = 0.0

# 학습 설정
lr = 0.25 #학습률
max_iter = 50 #최대반복횟수

for i in range(max_iter + 1):
    y_pred = w * x + b
    error = y_pred - y

    grad_w = (2 / len(x)) * np.dot(error, x)
    grad_b = (2 / len(x)) * np.sum(error)

    w -= lr * grad_w
    b -= lr * grad_b

    mse = np.mean(error ** 2)
    if i % 10 == 0 or i == 0:
        print(f"n = {i},  En = {mse:.4f}, w = {w:.4f}, b = {b:.4f}")

"""#### [단계 3] Linear Regression 결과 그리기

"""

plt.scatter(x, y, label='Noisy Data')
plt.plot(x, true_y, '--', label='True Line (y = 0.5x + 0.8)')
plt.plot(x, w * x + b, 'r-', label=f'Fitted Line (w={w:.2f}, b={b:.2f})')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Linear Regression Result')
plt.legend()
plt.grid(True)
plt.show()

"""## **문제 2: 이차원 분류기 (2-D Classifier)**
### 주어진 데이터에 대해 로지스틱 함수와 단층 구조를 이용하여 이차원 분류기 $𝜙(𝑤𝑥+𝑏)$를 구현하라.
### 최대 반복수(max interation)는 10,000으로 고정하고 학습률(learning rate)은 1로 정한다.
### 데이터는 산점도로 나타내고 분류함수에 대한 선형결정경계를 등위곡선으로 그려라.

#### [단계 1] 2-D Classification을 위한 훈련 Data (AND Logic) 생성 및 산점도 그리기
"""

import numpy as np
import matplotlib.pyplot as plt

# AND 논리 데이터
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,0,0,1])

plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', s=100, edgecolors='k')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('AND Logic Data')
plt.grid(True)
plt.show()

"""#### [단계 2]
#### - 로지스틱(시그모이드) 함수 정의
#### - 신경회로망 모델 파라미터 초기화
#### - 반복(iteration)루프 안에서 경사하강법을 이용하여 파라미터 업데이트 및 Cost Function (MSE) 평가
"""

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 파라미터 초기화
w = np.zeros(2)
b = 0.0
lr = 1
max_iter = 10000

for i in range(max_iter):
    z = np.dot(X, w) + b
    y_pred = sigmoid(z)
    error = y_pred - y
    grad_w = np.dot(error, X) / len(X)
    grad_b = np.mean(error)
    w -= lr * grad_w
    b -= lr * grad_b
    if i % 2000 == 0:
        mse = np.mean(error**2)
        print(f"iter={i}, MSE={mse:.4f}")

"""#### [단계 3] 이차원 Classifier의 Decision boundary와 Data points 그리기"""

xx, yy = np.meshgrid(np.linspace(-0.2,1.2,100), np.linspace(-0.2,1.2,100))
zz = sigmoid(w[0]*xx + w[1]*yy + b)

plt.contourf(xx, yy, zz, levels=[0,0.5,1], alpha=0.2, colors=['blue','red'])
plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', s=100, edgecolors='k')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('AND Logic Decision Boundary')
plt.grid(True)
plt.show()

"""## **문제 3:**

### 주어진 데이터에 대해 로지스틱 함수와 복층 구조를 이용하여 이차원 분류기를 구현하라.
### 은닉층의 노드를 2개로 두고, 최대 반복수는 10,000으로 고정하고 학습률은 1로 정한다.
### 데이터는 산점도로 나타내고 분류함수에 대한 경계를 등위곡선으로 그려라.

#### [단계 1] 2-D Classification을 위한 훈련 Data (XOR Logic) 생성 및 산점도 그리기
"""

X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,1,1,0])

plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', s=100, edgecolors='k')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('XOR Logic Data')
plt.grid(True)
plt.show()

"""#### [단계 2]
#### - 로지스틱(시그모이드) 함수 정의
#### - 신경회로망 모델 파라미터 초기화
#### - 반복(iteration)루프 안에서 경사하강법을 이용하여 파라미터 업데이트 및 Cost Function (MSE) 평가
"""

#시그모이드 함수 정의
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 파라미터 초기화
np.random.seed(0)
W1 = np.random.randn(2,2)
b1 = np.zeros(2)
W2 = np.random.randn(2)
b2 = 0.0
lr = 1
max_iter = 10000

for i in range(max_iter):
    # 순전파->파라미터 예측값 계산을 위해 필요함
    z1 = np.dot(X, W1) + b1
    a1 = sigmoid(z1)
    z2 = np.dot(a1, W2) + b2
    y_pred = sigmoid(z2)
    # 역전파
    error = y_pred - y
    dW2 = np.dot(a1.T, error * y_pred * (1 - y_pred)) / len(X)
    db2 = np.mean(error * y_pred * (1 - y_pred))
    dz1 = np.outer(error * y_pred * (1 - y_pred), W2) * a1 * (1 - a1)
    dW1 = np.dot(X.T, dz1) / len(X)
    db1 = np.mean(dz1, axis=0)
    # 업데이트
    W2 -= lr * dW2
    b2 -= lr * db2
    W1 -= lr * dW1
    b1 -= lr * db1
    if i % 2000 == 0:
        mse = np.mean(error**2)
        print(f"iter={i}, MSE={mse:.4f}")

"""#### [단계 3] 이차원 Classifier의 Decision boundary와 Data points 그리기"""

xx, yy = np.meshgrid(np.linspace(-0.2,1.2,100), np.linspace(-0.2,1.2,100))
grid = np.c_[xx.ravel(), yy.ravel()]
z1 = np.dot(grid, W1) + b1
a1 = sigmoid(z1)
z2 = np.dot(a1, W2) + b2
zz = sigmoid(z2).reshape(xx.shape)

plt.contourf(xx, yy, zz, levels=[0,0.5,1], alpha=0.2, colors=['blue','red'])
plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', s=100, edgecolors='k')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('XOR Logic Decision Boundary')
plt.grid(True)
plt.show()

"""## **문제 4:**

### 주어진 데이터에 대해 로지스틱 함수와 복층 구조를 이용하여 이차원 다중 분류기를 구현하라.
### 은닉층의 노드를 2개로 두고, 최대 반복수는 10,000으로 고정하고 학습률은 1로 정한다.
### 단, 편미분(그래디언트)는 반복문으로 계산하라.
### 데이터는 산점도로 나타내고 분류함수에 대한 경계를 등위곡선으로 그려라.

#### [단계 1] 2-D 다중 Classification을 위한 훈련 Data 생성 및 산점도 그리기
"""



"""#### [단계 2]
#### - 로지스틱(시그모이드) 함수 정의
#### - 신경회로망 모델 파라미터 초기화
#### - 반복(iteration)루프 안에서 경사하강법을 이용하여 파라미터 업데이트 및 Cost Function (MSE) 평가
"""



"""#### [단계 3] 이차원 다중 Classifier의 Decision boundary와 Data points 그리기"""



"""## **문제 5 (*Optional*):**

### 주어진 데이터에 대해 로지스틱(sigmoid) 함수와 복층 구조를 이용하여 이차원 다중 분류기를 구현하라.
### 은닉층 노드를 7개로 두고, 최대 반복수는 10,000으로 고정하고 학습률은 1로 정한다.
### 단, 편미분(그래디언트)는 반복문으로 계산하라.
### 데이터는 산점도로 나타내고 분류함수에 대한 경계를 등위곡선으로 그려라.

#### [단계 1] 2-D 다중 Classification을 위한 훈련 Data 생성 및 산점도 그리기
"""



"""#### [단계 2]
#### - 로지스틱(시그모이드) 함수 정의
#### - 신경회로망 모델 파라미터 초기화
#### - 반복(iteration)루프 안에서 경사하강법을 이용하여 파라미터 업데이트 및 Cost Function (MSE) 평가
"""



"""#### [단계 3] 이차원 다중 Classifier의 Decision boundary와 Data points 그리기"""



